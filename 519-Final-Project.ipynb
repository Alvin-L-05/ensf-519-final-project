{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4b2653",
   "metadata": {},
   "source": [
    "# ENSF 519 Final Project - Image to Caption Matcher\n",
    "### Project Description: Our project outlined in the steps below tests 2 deep learning deep learning model's ability to match images to image captions using the Flickr8k Dataset, then intakes a defined image and displays the closest predicted captions.\n",
    "Description: This project implements and compares two approaches for image-to-caption association: a pre-trained CLIP model (zero-shot) and a custom deep learning model trained on a Flickr-style dataset. The workflow includes loading and preprocessing images and captions, building a vocabulary, creating datasets and data loaders, defining image and text encoders (ResNet for images, BiLSTM for text), and training the custom model using a contrastive loss to align embeddings. After training, both models are evaluated on a held-out test set using retrieval metrics (Recall@1, Recall@5, Recall@10), and the results are compared. Finally, the project demonstrates the retrieval capability with a live demo image, retrieving the most relevant captions according to the models.\n",
    "## Team Members:\n",
    "- Gashagaza, Gisa Tchaka\n",
    "- Ghattas, Mohamad\n",
    "- Long, Alvin\n",
    "- van Roessel, Spencer\n",
    "\n",
    "How to run: To run the project, download the Flickr8k database, unzip the dataset, and place the \"captions.txt\" file as well as the \"Images\" folder into the same dirctory as the project file. The following packages need to be installed before running the code cells: \n",
    "- numpy \n",
    "- pillow \n",
    "- tqdm \n",
    "- torch \n",
    "- torchvision \n",
    "- transformers \n",
    "- nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a340969",
   "metadata": {},
   "source": [
    "## Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in the folder that contains: captions.txt  and Images/ (Flickr8k)\n",
    "# Requires: numpy, torch, torchvision, transformers, tqdm, nltk, pillow\n",
    "\n",
    "import os, math, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# HuggingFace CLIP\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360728a8",
   "metadata": {},
   "source": [
    "## Step 1: Setup Config / Paths / Device variables\n",
    "Set up the core dataset paths and computing environment, ensure the model can access the images, captions, and uploaded demo image while selecting the appropriate device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f109e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('.')    # project folder with \"captions.txt\" and \"Images\" folder, currently in the project directory\n",
    "CAPTIONS_FILE = DATA_DIR / 'captions.txt'\n",
    "IMAGES_DIR = DATA_DIR / 'Images'\n",
    "UPLOADED_IMAGE = \"people-playing-foosball.png\"  # uploaded image (for demo)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ed7ef",
   "metadata": {},
   "source": [
    "# Step 2: Helpers: load captions (CSV style)\n",
    "Load and structure the dataset captions by reading the captions file and map each image to its list of associated captions. Also prepare the text data in a convenient format for later training and evaluation of the image-to-caption models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_csv(captions_file):\n",
    "    # returns dict: image_filename -> list of captions\n",
    "    img2caps = defaultdict(list)\n",
    "    with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "        header = f.readline()  # skip header\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # split only on first comma\n",
    "            img, cap = line.split(',', 1)\n",
    "            img2caps[img].append(cap.strip())\n",
    "    return dict(img2caps)\n",
    "\n",
    "img2caps = load_captions_csv(CAPTIONS_FILE)\n",
    "print(\"Images with captions:\", len(img2caps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161f3c0",
   "metadata": {},
   "source": [
    "## Step 3: Build splits (train/val/test)\n",
    "Randomly split the dataset into training, validation, and tests sets to support proper model development and evaluation. Ensure that each model is trained on one subset of images while being fairly assessed on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638081b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = sorted(list(img2caps.keys()))\n",
    "random.seed(42)\n",
    "random.shuffle(all_images)\n",
    "N = len(all_images)\n",
    "n_test = max(500, int(0.1 * N))   # test ~10% or 500 images (can adjust)\n",
    "n_val = max(500, int(0.1 * N))\n",
    "test_imgs = all_images[:n_test]\n",
    "val_imgs = all_images[n_test:n_test+n_val]\n",
    "train_imgs = all_images[n_test+n_val:]\n",
    "print(\"Train/Val/Test sizes:\", len(train_imgs), len(val_imgs), len(test_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b837e7b",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing transforms\n",
    "Define the preprocessing pipeline for images, resizing them to a consistent size and normalizing their pixel values. Ensure that all images are formatted correctly and uniformly before being fed into the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0403b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "img_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5da95",
   "metadata": {},
   "source": [
    "## Step 5: Dataset to produce image + single caption sample\n",
    "Create a dataset that for each image returns a random caption among the 5. This stage defines a custom dataset class that loads image–caption pairs and applies preprocessing, enabling the model to receive training samples in a consistent format. It then creates the training, validation, and test datasets that will be used throughout the model development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44343d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrPairsDataset(Dataset):\n",
    "    def __init__(self, img_list, img2caps, images_dir, transform, max_text_len=30):\n",
    "        self.img_list = img_list\n",
    "        self.img2caps = img2caps\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform = transform\n",
    "        self.max_text_len = max_text_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = self.images_dir / img_name\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # choose one caption randomly for training sample\n",
    "        caption = random.choice(self.img2caps[img_name])\n",
    "        return image, caption, img_name\n",
    "\n",
    "# quick datasets\n",
    "train_ds = FlickrPairsDataset(train_imgs, img2caps, IMAGES_DIR, img_transform)\n",
    "val_ds   = FlickrPairsDataset(val_imgs, img2caps, IMAGES_DIR, img_transform)\n",
    "test_pairs = [(IMAGES_DIR / img, img2caps[img], img) for img in test_imgs]  # use all captions for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc68777",
   "metadata": {},
   "source": [
    "## Step 6: Utility: tokenize captions (simple nltk tokenizer)\n",
    "A simple text-processing function that lowercases each caption and splits it into individual tokens using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(s):\n",
    "    return nltk.word_tokenize(s.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cadc6",
   "metadata": {},
   "source": [
    "## 7a. Model A: CLIP baseline (HuggingFace)\n",
    "- We will use CLIP to compute embeddings for all images and all captions\n",
    "- Retrieval: cosine similarity between image and text embeddings\n",
    "\n",
    "This stage loads the pre-trained CLIP model and its processor, setting it up on the appropriate device for inference. It also defines a function to compute normalized embeddings for images and captions in batches, which will be used for tasks like similarity computation or retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21688b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading CLIP model...\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_clip_embeddings(image_paths, texts, batch_size=64):\n",
    "    # image_paths: list of PIL images or tensors/paths\n",
    "    # texts: list of text strings (same length) -- for dataset-wide embeddings\n",
    "    # returns two arrays: image_embs (N, D), text_embs (N, D)\n",
    "    # process images and texts separately\n",
    "    # if image_paths are PIL paths, open them\n",
    "    imgs = []\n",
    "    for p in image_paths:\n",
    "        if isinstance(p, (str, Path)):\n",
    "            im = Image.open(p).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
    "            imgs.append(im)\n",
    "        else:\n",
    "            imgs.append(p)  # already PIL or tensor\n",
    "    # batch process images\n",
    "    all_img_embs = []\n",
    "    for i in range(0, len(imgs), batch_size):\n",
    "        batch = imgs[i:i+batch_size]\n",
    "        proc = clip_processor(images=batch, return_tensors='pt').to(DEVICE)\n",
    "        out = clip_model.get_image_features(**proc)\n",
    "        emb = out.detach()\n",
    "        emb = F.normalize(emb, dim=-1)\n",
    "        all_img_embs.append(emb.cpu())\n",
    "    all_img_embs = torch.cat(all_img_embs, dim=0)\n",
    "    # texts\n",
    "    all_txt_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        proc = clip_processor(text=batch, return_tensors='pt', padding=True).to(DEVICE)\n",
    "        out = clip_model.get_text_features(**proc)\n",
    "        emb = F.normalize(out.detach(), dim=-1)\n",
    "        all_txt_embs.append(emb.cpu())\n",
    "    all_txt_embs = torch.cat(all_txt_embs, dim=0)\n",
    "    return all_img_embs.numpy(), all_txt_embs.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318de3a",
   "metadata": {},
   "source": [
    "## 7b. Model B: Custom model (ResNet18 + BiLSTM) to produce embeddings\n",
    "- We'll use pretrained ResNet18 (optionally freeze), project to embed_dim\n",
    "- Text encoder: word embedding + BiLSTM -> take last hidden (or mean) -> projection\n",
    "- Train with symmetric contrastive (cross-entropy on similarity logits)\n",
    "\n",
    "This stage defines custom deep learning encoders: a ResNet-based image encoder to extract visual features and a BiLSTM-based text encoder to convert tokenized captions into embeddings. Both models project their outputs into a shared embedding space and normalize them, enabling direct comparison between images and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0737a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoderResNet(nn.Module):\n",
    "    def __init__(self, embed_dim=512, train_resnet=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        # remove final fc\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])    # outputs (B,512,1,1)\n",
    "        if not train_resnet:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.proj = nn.Linear(resnet.fc.in_features, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B,3,H,W)\n",
    "        feat = self.backbone(x).squeeze(-1).squeeze(-1) # (B,512)\n",
    "        out = self.proj(feat)                               # (B,embed_dim)\n",
    "        out = F.normalize(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "class TextEncoderBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab, embed_dim=512, hidden=512, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(len(vocab)+1, embedding_dim=300, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=hidden//2, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.proj = nn.Linear(hidden, embed_dim)\n",
    "    \n",
    "    def forward(self, token_ids, lengths):\n",
    "        # token_ids: (B, T)\n",
    "        embedded = self.embedding(token_ids)            # (B,T,300)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, c_n) = self.lstm(packed)\n",
    "        # h_n: (num_layers*2, B, hidden//2)\n",
    "        h_cat = torch.cat([h_n[-2], h_n[-1]], dim=-1)   # (B, hidden)\n",
    "        out = self.proj(h_cat)                          # (B, embed_dim)\n",
    "        out = F.normalize(out, dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ad14f",
   "metadata": {},
   "source": [
    "## Step 8: Simple text tokenizer + vocab builder (small)\n",
    "Build a vocabulary from the training captions, keeping words that meet a minimum frequency and assigning unique token IDs, with special handling for padding and unknown words. Also defines a function to convert captions into fixed-length sequences of token IDs for input into the text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocab(img2caps, min_freq=2, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for caps in img2caps.values():\n",
    "        for c in caps:\n",
    "            toks = simple_tokenize(c)\n",
    "            counter.update(toks)\n",
    "    words = [w for w,c in counter.items() if c >= min_freq]\n",
    "    words = sorted(words, key=lambda w: -counter[w])[:max_size]\n",
    "    stoi = {w:i+1 for i,w in enumerate(words)}  # 0 reserved for pad\n",
    "    stoi['<unk>'] = len(stoi)+1\n",
    "    itos = {i:w for w,i in stoi.items()}\n",
    "    return {'stoi': stoi, 'itos': itos, 'pad': 0}\n",
    "\n",
    "vocab = build_vocab({k:img2caps[k] for k in train_imgs}, min_freq=2)\n",
    "print(\"Vocab size:\", len(vocab['stoi']))\n",
    "\n",
    "# convert a caption to token ids\n",
    "def caption_to_ids(caption, stoi, max_len=30):\n",
    "    toks = simple_tokenize(caption)[:max_len]\n",
    "    ids = [stoi.get(t, stoi['<unk>']) for t in toks]\n",
    "    length = len(ids)\n",
    "    if length < max_len:\n",
    "        ids = ids + [0] * (max_len - length)\n",
    "    return ids, length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89c4cd",
   "metadata": {},
   "source": [
    "## Step 9: DataLoader for training custom model\n",
    "Will yield batches of images and token_ids + lengths. Define a dataset loader for training that returns preprocessed images along with their tokenized caption IDs and lengths. Also provide a collate function to batch the samples efficiently for input into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPairsLoader(Dataset):\n",
    "    def __init__(self, img_list, img2caps, images_dir, transform, stoi, max_len=30):\n",
    "        self.img_list = img_list\n",
    "        self.img2caps = img2caps\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform = transform\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_list[idx]\n",
    "        image = Image.open(self.images_dir / img).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = random.choice(self.img2caps[img])\n",
    "        ids, length = caption_to_ids(caption, self.stoi, max_len=self.max_len)\n",
    "        return image, torch.tensor(ids, dtype=torch.long), length\n",
    "\n",
    "def collate_train(batch):\n",
    "    images, ids_list, lengths = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    ids = torch.stack(ids_list, dim=0)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return images, ids, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0776350",
   "metadata": {},
   "source": [
    "## Step 10: Contrastive loss training: symmetric cross-entropy on similarity logits\n",
    "Define a contrastive loss function that measures how well image and text embeddings align in a shared space. En matching image–caption pairs to have high similarity while pushing non-matching pairs apart, using a temperature-scaled cross-entropy formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_from_embeddings(img_emb, txt_emb, temperature=0.07):\n",
    "    # img_emb: (B,D), txt_emb: (B,D)\n",
    "    logits = img_emb @ txt_emb.t()       # (B,B)\n",
    "    logits = logits / temperature\n",
    "    targets = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i = F.cross_entropy(logits, targets)\n",
    "    loss_t = F.cross_entropy(logits.t(), targets)\n",
    "    return (loss_i + loss_t) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bd0d8",
   "metadata": {},
   "source": [
    "## Step 11: Training loop for custom model (small epochs for test)\n",
    "Implement the training loop for the custom image and text encoders, computing embeddings, applying the contrastive loss, and updating model parameters via backpropagation. Iterate over the training dataset for multiple epochs, tracking and reporting the average loss to monitor learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(image_encoder, text_encoder, train_loader, optimizer, epochs=3):\n",
    "    image_encoder.train(); text_encoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, ids, lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            images = images.to(DEVICE)\n",
    "            ids = ids.to(DEVICE)\n",
    "            lengths = lengths.to(DEVICE)\n",
    "            img_emb = image_encoder(images)                       # (B,D)\n",
    "            txt_emb = text_encoder(ids, lengths)                  # (B,D)\n",
    "            loss = contrastive_loss_from_embeddings(img_emb, txt_emb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} avg loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ae8b6",
   "metadata": {},
   "source": [
    "## Step 12: Retrieval evaluation metrics\n",
    "Compute retrieval evaluation metrics to assess how well the image and text embeddings align. Calculate ranks of the correct captions for each image and reports Recall@1, Recall@5, Recall@10, and the median rank to quantify retrieval performance.\n",
    "- Recall@1: The percentage of images where a correct caption was in the top 1 of the model predictions.\n",
    "- Recall@5: The percentage of images where a correct caption was in the top 5 of the model predictions.\n",
    "- Recall@10: The percentage of images where a correct caption was in the top 10 of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_metrics(image_embs, text_embs, img_names, text_to_img_map):\n",
    "    # image_embs: (N_img, D), text_embs: (N_text, D)\n",
    "    # img_names: list of image file names corresponding to image_embs\n",
    "    # text_to_img_map: list mapping each text index -> image index\n",
    "    sims = image_embs @ text_embs.T   # (N_img, N_text)\n",
    "    # For each image, rank texts by similarity\n",
    "    ranks = []\n",
    "    for i in range(sims.shape[0]):\n",
    "        sim_row = sims[i]\n",
    "        sorted_idx = np.argsort(-sim_row)  # descending\n",
    "        # find rank of the first *correct* text (any of the 5 captions)\n",
    "        correct_text_inds = [j for j,t in enumerate(text_to_img_map) if t == i]\n",
    "        # find best rank among correct_text_inds\n",
    "        rank_positions = [np.where(sorted_idx == ct)[0][0] for ct in correct_text_inds]\n",
    "        ranks.append(min(rank_positions) + 1)  # ranks are 1-based\n",
    "    ranks = np.array(ranks)\n",
    "    r1 = np.mean(ranks <= 1) * 100\n",
    "    r5 = np.mean(ranks <= 5) * 100\n",
    "    r10 = np.mean(ranks <= 10) * 100\n",
    "    med = np.median(ranks)\n",
    "    return {'R@1': r1, 'R@5': r5, 'R@10': r10, 'Median Rank': med}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af685055",
   "metadata": {},
   "source": [
    "## Step 13: Precompute caption list for retrieval\n",
    "We'll prepare one caption per image entry in text_embs list (i.e., 5 captions per image). Prepare the test set for retrieval evaluation by collecting all captions and mapping each one to its corresponding image. Create lists of captions, their associated image indices, and image names to be used for computing similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85217665",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "text_to_img = []\n",
    "text_img_names = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    caps = img2caps[img]\n",
    "    for c in caps:\n",
    "        all_texts.append(c)\n",
    "        text_to_img.append(i)   # index in test_imgs\n",
    "        text_img_names.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17837fbb",
   "metadata": {},
   "source": [
    "## Step 14: Evaluate CLIP baseline on test set\n",
    "Evaluate the pre-trained CLIP model on the test set by computing embeddings for all test images and captions. Then, calculate retrieval metrics to measure how well CLIP matches images to their corresponding captions, providing a baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f76ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clip_on_test(test_imgs, all_texts, text_to_img):\n",
    "    # prepare image list (PIL)\n",
    "    img_paths = [IMAGES_DIR / img for img in test_imgs]\n",
    "    # compute CLIP embs\n",
    "    print(\"Computing CLIP embeddings...\")\n",
    "    img_embs, txt_embs = compute_clip_embeddings(img_paths, all_texts, batch_size=64)\n",
    "    print(\"Image embs:\", img_embs.shape, \"Text embs:\", txt_embs.shape)\n",
    "    # compute retrieval metrics (image->text)\n",
    "    metrics = retrieval_metrics(img_embs, txt_embs, test_imgs, text_to_img)\n",
    "    return metrics\n",
    "\n",
    "# Run CLIP baseline (this is faster because model is pretrained)\n",
    "clip_metrics = eval_clip_on_test(test_imgs, all_texts, text_to_img)\n",
    "print(\"CLIP baseline metrics (image->text):\", clip_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf17b5",
   "metadata": {},
   "source": [
    "## Step 15: Prepare and train custom model\n",
    "Set up a smaller subset of the training data for fast experimentation, initialize the custom image and text encoders, and define the optimizer. Next, run a quick training loop to fine-tune the models on this subset, allowing for testing of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec741230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small subset for quick testing (comment out for full training)\n",
    "subset_train_imgs = train_imgs[:2000]  # reduce if you want a fast run\n",
    "train_loader = DataLoader(TrainPairsLoader(subset_train_imgs, img2caps, IMAGES_DIR, img_transform, vocab['stoi']),\n",
    "                          batch_size=64, shuffle=True, collate_fn=collate_train)\n",
    "\n",
    "embed_dim = 512\n",
    "image_encoder = ImageEncoderResNet(embed_dim=embed_dim, train_resnet=True).to(DEVICE)\n",
    "text_encoder = TextEncoderBiLSTM(vocab=vocab['stoi'], embed_dim=embed_dim, hidden=512).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(image_encoder.parameters()) + list(text_encoder.parameters()), lr=1e-4)\n",
    "\n",
    "# Quick train (few epochs)\n",
    "train_custom_model(image_encoder, text_encoder, train_loader, optimizer, epochs=10)  # Currently only running for 2 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b8405",
   "metadata": {},
   "source": [
    "## Step 16: Compute embeddings for test set for custom model\n",
    "Compute embeddings for the test images and captions using the fine-tuned custom image and text encoders. Evaluate the model’s retrieval performance on the test set by calculating metrics, providing a comparison to the CLIP baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb949bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_custom_embeddings(image_paths, texts, image_encoder, text_encoder, stoi, batch_size=64, max_len=30):\n",
    "    # image_paths: list of image Path objects\n",
    "    image_embs = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_imgs = [Image.open(p).convert('RGB') for p in batch_paths]\n",
    "        batch_imgs = [img_transform(im) for im in batch_imgs]\n",
    "        batch = torch.stack(batch_imgs).to(DEVICE)\n",
    "        emb = image_encoder(batch).cpu()\n",
    "        image_embs.append(emb)\n",
    "    image_embs = torch.cat(image_embs, dim=0).numpy()\n",
    "    \n",
    "    text_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        ids = []\n",
    "        lengths = []\n",
    "        for t in batch:\n",
    "            idseq, L = caption_to_ids(t, stoi, max_len=max_len)\n",
    "            ids.append(idseq)\n",
    "            lengths.append(L)\n",
    "        ids = torch.tensor(ids, dtype=torch.long).to(DEVICE)\n",
    "        lengths = torch.tensor(lengths).to(DEVICE)\n",
    "        emb = text_encoder(ids, lengths).cpu()\n",
    "        text_embs.append(emb)\n",
    "    text_embs = torch.cat(text_embs, dim=0).numpy()\n",
    "    return image_embs, text_embs\n",
    "\n",
    "print(\"Computing custom model embeddings (test set)...\")\n",
    "img_paths = [IMAGES_DIR / img for img in test_imgs]\n",
    "custom_img_embs, custom_txt_embs = compute_custom_embeddings(img_paths, all_texts, image_encoder, text_encoder, vocab['stoi'])\n",
    "custom_metrics = retrieval_metrics(custom_img_embs, custom_txt_embs, test_imgs, text_to_img)\n",
    "print(\"Custom model metrics (image->text):\", custom_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b46e9",
   "metadata": {},
   "source": [
    "## Step 17: Compare and print results\n",
    "This stage summarizes and compares the retrieval performance of the pre-trained CLIP model and the fine-tuned custom model. It also prints the key metrics (Recall@1, Recall@5, Recall@10) to highlight how training improves image-to-caption matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfff83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Results ===\")\n",
    "print(\"CLIP (zero-shot) R@1/5/10:\", clip_metrics['R@1'], clip_metrics['R@5'], clip_metrics['R@10'])\n",
    "print(\"Custom (trained) R@1/5/10:\", custom_metrics['R@1'], custom_metrics['R@5'], custom_metrics['R@10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d868ff9",
   "metadata": {},
   "source": [
    "# Step 18: Quick qualitative demo with uploaded image\n",
    "As an add-on, this step demonstrates the CLIP model’s zero-shot retrieval by displaying an uploaded image and finding the top matching captions from a subset of test captions. It computes the image embedding, compares it to precomputed text embeddings, and outputs the highest-similarity captions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485776cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(Image.open(UPLOADED_IMAGE))  # show uploaded image\n",
    "\n",
    "# Use CLIP to retrieve top captions among all_texts for that uploaded image\n",
    "proc = clip_processor(images=Image.open(UPLOADED_IMAGE).convert('RGB').resize((IMG_SIZE, IMG_SIZE)), return_tensors='pt').to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    im_feat = clip_model.get_image_features(**proc)\n",
    "    im_feat = F.normalize(im_feat, dim=-1).cpu().numpy()  # (1,D)\n",
    "# compare to CLIP text embeddings we computed earlier (txt_embs from eval_clip_on_test)\n",
    "# Note: compute_clip_embeddings returns arrays; reload if needed. For demo, recompute small set:\n",
    "demo_texts = all_texts[:500]   # demo subset\n",
    "_, demo_txt_embs = compute_clip_embeddings([IMAGES_DIR / i for i in test_imgs[:500]], demo_texts, batch_size=64)\n",
    "sims = (im_feat @ demo_txt_embs.T).squeeze()\n",
    "topk = sims.argsort()[::-1][:10]\n",
    "print(\"Top CLIP captions:\")\n",
    "for k in topk:\n",
    "    print(demo_texts[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
